{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: increase size\n",
    "#TODO: change train to include labels, ignore for ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Dense, Conv2D, Conv2DTranspose , LeakyReLU, Dropout, Flatten, Input, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 64, 64, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, 64, 64, 3), (None, 64, 64, 3), (None,)), types: (tf.float32, tf.float32, tf.int64)>\n",
      "<MapDataset shapes: ((64, 64, 3), ()), types: (tf.float32, tf.int64)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 13:27:03.851961: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 13:27:03.857524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 13:27:03.857713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-26 13:27:03.861524: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#run for no gpu if needed\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "DATA_DIR='/home/evan/Datasets/tensorflow'\n",
    "BATCH_SIZE=32\n",
    "BUFFER_SIZE = 10000\n",
    "IMG_SIZE = (64,64)\n",
    "EMBEDDED_DIM = 32\n",
    "\n",
    "train_dataset = tfds.load('stanford_dogs', as_supervised=True, split='train', data_dir=DATA_DIR, download=False)\n",
    "test_dataset = tfds.load('stanford_dogs', as_supervised=True, split='test', data_dir=DATA_DIR, download=False)\n",
    "\n",
    "def transform(x , y):\n",
    "    x = tf.image.resize(x, IMG_SIZE)\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    x = (x - 127.5)/127.5\n",
    "    return x, y\n",
    "    \n",
    "# train_dataset = train_dataset.map(lambda x, y: (tf.cast(x, tf.uint8), y))\n",
    "\n",
    "def transform_ae(x,y):\n",
    "    return (x,x,y)\n",
    "\n",
    "def prepare_transformed(dataset):\n",
    "    return dataset.repeat().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(lambda x, y : transform(x,y))\n",
    "train_dataset = train_dataset.map(lambda x, y : transform(x,y))\n",
    "ae_dataset = train_dataset.map(lambda x, y : transform_ae(x,y))\n",
    "ae_dataset = prepare_transformed(ae_dataset)\n",
    "train_dataset = prepare_transformed(train_dataset)\n",
    "\n",
    "print(train_dataset)\n",
    "print(ae_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 16, 16, 128), dtype=float32)\n",
      "(None, 32, 32, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 32, 32, 128) dtype=float32 (created by layer 'conv2d_transpose_tied_41')>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Conv2DTransposeTied(Layer):\n",
    "    def __init__(self, conv2D, activation=None, **kwargs):\n",
    "        self.conv2D = conv2D\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, batch_input_shape):\n",
    "        self.biases = self.add_weight(name='bias', initializer ='zeros',\n",
    "                                      shape= self.conv2D.input_shape[1:]) #exclude batch dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        print(inputs)\n",
    "        print(self.conv2D.input_shape)\n",
    "        Z = tf.nn.conv2d_transpose(inputs, self.conv2D.kernel, (BATCH_SIZE,) + self.conv2D.input_shape[1:], self.conv2D.strides, padding=self.conv2D.padding.upper())\n",
    "        return self.activation(Z + self.biases)\n",
    "\n",
    "Conv2DTransposeTied(c3)(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 16, 16, 128), dtype=tf.float32, name=None), name='dropout_50/Identity:0', description=\"created by layer 'dropout_50'\")\n"
     ]
    }
   ],
   "source": [
    "encoder_input_img = Input(IMG_SIZE+((3,)))\n",
    "encoder_input_label = Input((1))\n",
    "c1 = Conv2D(64, (5, 5), strides=(1, 1), padding='same', input_shape=IMG_SIZE+((3,)))\n",
    "Z = c1(encoder_input_img)\n",
    "Z = LeakyReLU()(Z)\n",
    "Z = Dropout(0.3)(Z)\n",
    "\n",
    "c2 = Conv2D(128, (5, 5), strides=(2, 2), padding='same')\n",
    "Z = c2(Z)\n",
    "Z = LeakyReLU()(Z)\n",
    "Z = Dropout(0.3)(Z)\n",
    "\n",
    "c3 = Conv2D(128, (5, 5), strides=(2, 2), padding='same')\n",
    "Z = c3(Z)\n",
    "Z = LeakyReLU()(Z)\n",
    "Z = Dropout(0.3)(Z)\n",
    "print(Z)\n",
    "Z = Flatten()(Z)\n",
    "latent_embedding = Dense(EMBEDDED_DIM)(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Conv2DTransposeTied at 0x7f1542188190>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2DTransposeTied(c3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(None, 16, 16, 128), dtype=float32)\n",
      "(None, 32, 32, 128)\n",
      "Tensor(\"Placeholder:0\", shape=(None, 32, 32, 128), dtype=float32)\n",
      "(None, 64, 64, 64)\n",
      "Tensor(\"Placeholder:0\", shape=(None, 64, 64, 64), dtype=float32)\n",
      "(None, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 64, 64, 3) dtype=float32 (created by layer 'dropout_53')>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = Dense(16*16*128)(latent_embedding)\n",
    "Z = Reshape((16,16,128))(Z)\n",
    "\n",
    "\n",
    "c3T = Conv2DTransposeTied(c3)(Z)\n",
    "c3Ta = LeakyReLU()(c3T)\n",
    "c3Td = Dropout(0.3)(c3Ta)\n",
    "\n",
    "c2T = Conv2DTransposeTied(c2)(c3Td)\n",
    "c2Ta = LeakyReLU()(c2T)\n",
    "c2Td = Dropout(0.3)(c2Ta)\n",
    "\n",
    "c1T = Conv2DTransposeTied(c1)(c2T)\n",
    "c1Ta = LeakyReLU()(c1T)\n",
    "c1Td = Dropout(0.3)(c1Ta)\n",
    "\n",
    "decoder_output = c1Td\n",
    "\n",
    "autoencoder = Model(encoder_input_img, decoder_output)\n",
    "autoencoder.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer=tf.keras.optimizers.Adam(lr=.001)\n",
    "\n",
    "autoencoder.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Tensor(\"model/reshape_2/Reshape:0\", shape=(None, 16, 16, 128), dtype=float32)\n",
      "(None, 32, 32, 128)\n",
      "Tensor(\"model/dropout_51/dropout/Mul_1:0\", shape=(None, 32, 32, 128), dtype=float32)\n",
      "(None, 64, 64, 64)\n",
      "Tensor(\"model/conv2d_transpose_tied_44/add:0\", shape=(None, 64, 64, 64), dtype=float32)\n",
      "(None, 64, 64, 3)\n",
      "Tensor(\"model/reshape_2/Reshape:0\", shape=(None, 16, 16, 128), dtype=float32)\n",
      "(None, 32, 32, 128)\n",
      "Tensor(\"model/dropout_51/dropout/Mul_1:0\", shape=(None, 32, 32, 128), dtype=float32)\n",
      "(None, 64, 64, 64)\n",
      "Tensor(\"model/conv2d_transpose_tied_44/add:0\", shape=(None, 64, 64, 64), dtype=float32)\n",
      "(None, 64, 64, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 14:20:52.180258: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95/100 [===========================>..] - ETA: 2s - loss: 18.4457"
     ]
    }
   ],
   "source": [
    "autoencoder.fit(ae_dataset, epochs=100, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder.save('./autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "autoencoder = keras.models.load_model('/home/evan/Desktop/Convolutional Random Forest/autoencoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(train_dataset.take(1))\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(4, 8, i+1)\n",
    "    plt.imshow(((predictions[i, :, :, :]*127.5)+127.5)/255.)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dataset = train_dataset.map(lambda x,y : ((x,y),y))\n",
    "encoder = keras.Model([encoder_input_img,encoder_input_label],[encoder_output_img, encoder_input_label])\n",
    "print(rf_dataset)\n",
    "rf_dataset = encoder.predict(rf_dataset, steps=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dataset[0].shape\n",
    "rf_dataset[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_image_dataset = tf.data.Dataset.from_tensor_slices(rf_dataset[0])\n",
    "rf_label_dataset = tf.data.Dataset.from_tensor_slices(rf_dataset[1])\n",
    "rf_combined_dataset = tf.data.Dataset.zip(((rf_image_dataset),(rf_label_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "from tensorflow.keras import Sequential\n",
    "import keras\n",
    "\n",
    "#run encoder to get embeddings\n",
    "# (encoder_output)\n",
    "rf_model = tfdf.keras.RandomForestModel()\n",
    "rf_model.fit(rf_dataset[0],rf_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(rf_model, encoder, input):\n",
    "    embeddings = encoder.predict(input,tf.ones(input.shape[0]))\n",
    "    return rf_model.predict(embeddings)\n",
    "run_inference(rf_model, encoder, rf_dataset[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ef37928cbe5219b2fcf8404ea29b31b1a966345f86328e41ae7ac18a87552f8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
